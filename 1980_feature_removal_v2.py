# -*- coding: utf-8 -*-
"""1980_feature_removal_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lg4VJYfJZW_ftqMjL2u-758hKSF5Z7n_
"""

#mount drive
#from google.colab import drive
#drive.mount('/content/drive')

#install librosa
#!pip install librosa

#more imports
import librosa
from librosa import display
import numpy as np
import os
import pandas as pd
import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import tensorflow as tf
from matplotlib.pyplot import specgram
import tensorflow.keras
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding
from tensorflow.keras.layers import LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input, Flatten, Dropout, Activation
from tensorflow.keras.layers import Conv1D, MaxPooling1D, AveragePooling1D
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.metrics import confusion_matrix
from sklearn import preprocessing
import sys
import pickle
from sklearn.metrics import accuracy_score

#decision tree
def DecisionTree(X_train, X_test, y_train, y_test):

  dtree = DecisionTreeClassifier()
  dtree.fit(X_train, y_train)
  predictions = dtree.predict(X_test)
  accuracy = accuracy_score(y_test, predictions)
  print(classification_report(y_test,predictions))

  return dtree, accuracy

#random forest
def RandomForest(X_train, X_test, y_train, y_test):

  rforest = RandomForestClassifier(criterion="gini", max_depth=10, max_features="log2", 
                                   max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, 
                                   n_estimators= 22000, random_state= 5)

  rforest.fit(X_train, y_train)
  predictions = rforest.predict(X_test)
  accuracy = accuracy_score(y_test, predictions)
  print(classification_report(y_test,predictions))

  return rforest, accuracy

#neural network
def NeuralNetwork(X_train, X_test, y_train, y_test, classnum, label_type, dataset):

  x_traincnn = np.expand_dims(X_train, axis=2)
  x_testcnn = np.expand_dims(X_test, axis=2)

  #Begin constructing model layers
  model = Sequential()

  #layers
  model.add(Conv1D(128, 5,padding='same',
                   input_shape=(40,1)))
  model.add(Activation('relu'))
  model.add(Dropout(0.1))
  model.add(MaxPooling1D(pool_size=(8)))
  model.add(Conv1D(128, 5,padding='same',))
  model.add(Activation('relu'))
  model.add(Dropout(0.1))
  model.add(Flatten())

  #final classification layer
  model.add(Dense(classnum))
  model.add(Activation('softmax'))

  #optimizer
  opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005, rho=0.9, epsilon=None, decay=0.0)

  #summarize
  model.summary()

  #use categorical cross entropy
  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=opt,
                metrics=['accuracy'])

  #Due to differences in the dataset, each set requires a different onehot label encoder
  le = preprocessing.LabelEncoder()

  if(dataset == 'ravdess'):
    if(label_type == 'emotion'):
      le.fit(["01", "02", "03", "04", "05", "06", "07", "08"])
      #01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised
    elif(label_type == 'gender'):
      le.fit(["m","f"])
  elif(dataset == 'emodb'):
    le.fit(["F", "N", "W", "T", "A", "L", "E"])
  elif(dataset == 'emovo'):
    le.fit(["dis", "gio", "neu", "pau", "rab", "sor", "tri"])
  elif(dataset == 'savee'):
    le.fit(["a", "d", "f", "h", "n", "sa", "su"])
  

  #encode
  y_train_encoded = le.transform(y_train)
  y_test_encoded = le.transform(y_test)

  #fit model
  cnnhistory=model.fit(x_traincnn, y_train_encoded, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test_encoded))

  #best accuracy
  best_model_accuracy = cnnhistory.history['val_accuracy'][np.argmin(cnnhistory.history['loss'])]

  #full classification report
  predictions = np.argmax(model.predict(x_testcnn), axis = 1)
  print(classification_report(y_test_encoded, predictions))

  return model, cnnhistory, x_testcnn, best_model_accuracy

def explainTree(model,X):
  shap.initjs()
  explainer = shap.TreeExplainer(model)
  #emotional subset for X
  shap_values = explainer.shap_values(X)
  #print(shap_values)
  shap.summary_plot(shap_values, features=X, class_names=model.classes_, max_display= 40)

def explainNN(model,xTrain,xTest):
  shap.initjs()
  explainer = shap.KernelExplainer(model.predict,xTrain)
  shap_values = explainer.shap_values(xTest,nsamples=100)
  #print(shap_values)
  shap.summary_plot(shap_values, class_names=model.classes_, max_display= 40)

#RAVDESS
#path and file list
dataset_name = 'ravdess'
path = '/content/drive/My Drive/Senior/1980/' + dataset_name
lst = []

for subdir, dirs, files in os.walk(path):
    for file in files:
        try:
          #Load librosa array, obtain mfcss, store the file and the mcss information in a new array
          X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')
          mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) 
          
          #file1 - emotion; file2 - gender
          file1 = ""
          file2 = ""

          #emotion label
          file1 = file[6:8]

          num_male = 0
          num_female = 0

          #gender label
          if(int(file[-6:-4]) % 2 == 0):
            file2 = 'f'
            num_female = num_female + 1
          else:
            file2 = 'm'
            num_male = num_male + 1

          #append to list
          arr = mfccs, file1, file2
          lst.append(arr)

        # If the file is not valid, skip it
        except ValueError:
          continue

with open('ravdess_emo_gender.pkl', 'wb') as f:
  pickle.dump(lst, f)

#lists of accuracies
ravdess_rf_accuracies = np.zeros(5)
ravdess_dt_accuracies = np.zeros(5)
ravdess_nn_accuracies = np.zeros(5)

#list of models
treeModels = []
forestModels = []
nnModels = []

#features to remove
dt_features_list = [0,2,1,30,39,38,35,5,33,34,31,29,17,6,4,18,28,8,19,21]
rf_features_list = [0,30,39,35,6,34,38,17,37,12,15,2,16,33,36,1,32,14,25,20]
nn_features_list = [2,4,39,38,19,17,1,33,0,7,34,18,20,9,37,10,15,13,11,31]

#get full arrays
X, y_emotion, y_gender = zip(*lst)
X = np.asarray(X)
y_emotion = np.asarray(y_emotion)
y_gender = np.asarray(y_gender)

#split into training and testing data
X_train, X_test, y_train_emo, y_test_emo, y_train_gender, y_test_gender = train_test_split(X, y_emotion, y_gender,test_size=0.33,random_state=42)

#for each set of features removed
for i in range(0, 10, 2):

  #indices of features removed
  dt_ind = dt_features_list[0:i]
  rf_ind = rf_features_list[0:i]
  nn_ind = nn_features_list[0:i]

  #modified training data
  X_mod_dt = X.copy()
  X_mod_rf = X.copy()
  X_mod_nn = X.copy()

  #remove features
  X_mod_dt[:,dt_ind] = 0
  X_mod_rf[:,rf_ind] = 0
  X_mod_nn[:,nn_ind] = 0

  #split into training and testing data
  X_train_dt, X_test_dt, y_train_emo_dt, y_test_emo_dt, y_train_gender_dt, y_test_gender_dt = train_test_split(X_mod_dt, y_emotion, y_gender, test_size=0.33,random_state=123)
  X_train_rf, X_test_rf, y_train_emo_rf, y_test_emo_rf, y_train_gender_rf, y_test_gender_rf = train_test_split(X_mod_rf, y_emotion, y_gender, test_size=0.33,random_state=123)
  X_train_nn, X_test_nn, y_train_emo_nn, y_test_emo_nn, y_train_gender_nn, y_test_gender_nn = train_test_split(X_mod_nn, y_emotion, y_gender, test_size=0.33,random_state=123)

  #train models
  treeModel, dt_acc = DecisionTree(X_train_dt, X_test_dt, y_train_emo_dt, y_test_emo_dt)
  rfModel, rf_acc = RandomForest(X_train_rf, X_test_rf, y_train_emo_rf, y_test_emo_rf)
  nnModel, nnHistory, x_testcnn, nn_acc = NeuralNetwork(X_train_nn, X_test_nn, y_train_emo_nn, y_test_emo_nn, 8, "emotion", 'ravdess')

  #add to list
  ravdess_dt_accuracies[int(i/2)] = dt_acc
  ravdess_rf_accuracies[int(i/2)] = rf_acc
  ravdess_nn_accuracies[int(i/2)] = nn_acc
  treeModels.append(treeModel)
  forestModels.append(rfModel)
  nnModels.append(nnModel)

print(ravdess_dt_accuracies)
print(ravdess_rf_accuracies)
print(ravdess_nn_accuracies)

#EmoDB
#path and file list
dataset_name = 'EmoDB'
path = '/content/drive/My Drive/Senior/1980/' + dataset_name
lst = []

for subdir, dirs, files in os.walk(path):
    for file in files:
        try:
          #Load librosa array, obtain mfcss, store the file and the mcss information in a new array
          X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')
          mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) 
          #The emotion label for each audio file is saved in a different portion of the filename for
          #each dataset. Must subset appropriately
          file = file[5]
          
          arr = mfccs, file
          lst.append(arr)
        # If the file is not valid, skip it
        except ValueError:
          continue

with open('emodb.pkl', 'wb') as f:
  pickle.dump(lst, f)

#lists of accuracies
emodb_rf_accuracies = np.zeros(5)
emodb_dt_accuracies = np.zeros(5)
emodb_nn_accuracies = np.zeros(5)

#list of models
emodb_treeModels = []
emodb_forestModels = []
emodb_nnModels = []

#features to remove
dt_features_list = [0,2,1,30,39,38,35,5,33,34,31,29,17,6,4,18,28,8,19,21]
rf_features_list = [0,30,39,35,6,34,38,17,37,12,15,2,16,33,36,1,32,14,25,20]
nn_features_list = [2,4,39,38,19,17,1,33,0,7,34,18,20,9,37,10,15,13,11,31]

#get full arrays
X, y_emotion = zip(*lst)
X = np.asarray(X)
y_emotion = np.asarray(y_emotion)

X_train, X_test, y_train_emo, y_test_emo = train_test_split(X, y_emotion,test_size=0.33,random_state=42)

#for each set of features removed
for i in range(0, 10, 2):

  #indices of features removed
  dt_ind = dt_features_list[0:i]
  rf_ind = rf_features_list[0:i]
  nn_ind = nn_features_list[0:i]

  #modified training data
  X_mod_dt = X.copy()
  X_mod_rf = X.copy()
  X_mod_nn = X.copy()

  #remove features
  X_mod_dt[:,dt_ind] = 0
  X_mod_rf[:,rf_ind] = 0
  X_mod_nn[:,nn_ind] = 0

  #split into training and testing data
  X_train_dt, X_test_dt, y_train_emo_dt, y_test_emo_dt = train_test_split(X_mod_dt, y_emotion, test_size=0.33,random_state=123)
  X_train_rf, X_test_rf, y_train_emo_rf, y_test_emo_rf = train_test_split(X_mod_rf, y_emotion,  test_size=0.33,random_state=123)
  X_train_nn, X_test_nn, y_train_emo_nn, y_test_emo_nn = train_test_split(X_mod_nn, y_emotion, test_size=0.33,random_state=123)

  #train models
  treeModel, dt_acc = DecisionTree(X_train_dt, X_test_dt, y_train_emo_dt, y_test_emo_dt)
  rfModel, rf_acc = RandomForest(X_train_rf, X_test_rf, y_train_emo_rf, y_test_emo_rf)
  nnModel, nnHistory, x_testcnn, nn_acc = NeuralNetwork(X_train_nn, X_test_nn, y_train_emo_nn, y_test_emo_nn, 7, "emotion", 'emodb')

  #add to list
  emodb_dt_accuracies[int(i/2)] = dt_acc
  emodb_rf_accuracies[int(i/2)] = rf_acc
  emodb_nn_accuracies[int(i/2)] = nn_acc
  emodb_treeModels.append(treeModel)
  emodb_forestModels.append(rfModel)
  emodb_nnModels.append(nnModel)

print(emodb_dt_accuracies)
print(emodb_rf_accuracies)
print(emodb_nn_accuracies)

#Emovo
#path and file list
dataset_name = 'emovo'
path = '/content/drive/My Drive/Senior/1980/' + dataset_name
lst = []

for subdir, dirs, files in os.walk(path):
    for file in files:
        try:
          #Load librosa array, obtain mfcss, store the file and the mcss information in a new array
          X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')
          mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) 
          #The emotion label for each audio file is saved in a different portion of the filename for
          #each dataset. Must subset appropriately
          file = file[0:3]
          
          arr = mfccs, file
          lst.append(arr)
        # If the file is not valid, skip it
        except ValueError:
          continue

with open('emovo.pkl', 'wb') as f:
  pickle.dump(lst, f)

#lists of accuracies
emovo_rf_accuracies = np.zeros(5)
emovo_dt_accuracies = np.zeros(5)
emovo_nn_accuracies = np.zeros(5)

#list of models
emovo_treeModels = []
emovo_forestModels = []
emovo_nnModels = []

#features to remove
dt_features_list = [0,2,1,30,39,38,35,5,33,34,31,29,17,6,4,18,28,8,19,21]
rf_features_list = [0,30,39,35,6,34,38,17,37,12,15,2,16,33,36,1,32,14,25,20]
nn_features_list = [2,4,39,38,19,17,1,33,0,7,34,18,20,9,37,10,15,13,11,31]

#get full arrays
X, y_emotion = zip(*lst)
X = np.asarray(X)
y_emotion = np.asarray(y_emotion)

X_train, X_test, y_train_emo, y_test_emo = train_test_split(X, y_emotion,test_size=0.33,random_state=42)

#for each set of features removed
for i in range(0, 10, 2):

  #indices of features removed
  dt_ind = dt_features_list[0:i]
  rf_ind = rf_features_list[0:i]
  nn_ind = nn_features_list[0:i]

  #modified training data
  X_mod_dt = X.copy()
  X_mod_rf = X.copy()
  X_mod_nn = X.copy()

  #remove features
  X_mod_dt[:,dt_ind] = 0
  X_mod_rf[:,rf_ind] = 0
  X_mod_nn[:,nn_ind] = 0

  #split into training and testing data
  X_train_dt, X_test_dt, y_train_emo_dt, y_test_emo_dt = train_test_split(X_mod_dt, y_emotion, test_size=0.33,random_state=123)
  X_train_rf, X_test_rf, y_train_emo_rf, y_test_emo_rf = train_test_split(X_mod_rf, y_emotion,  test_size=0.33,random_state=123)
  X_train_nn, X_test_nn, y_train_emo_nn, y_test_emo_nn = train_test_split(X_mod_nn, y_emotion, test_size=0.33,random_state=123)

  #train models
  treeModel, dt_acc = DecisionTree(X_train_dt, X_test_dt, y_train_emo_dt, y_test_emo_dt)
  rfModel, rf_acc = RandomForest(X_train_rf, X_test_rf, y_train_emo_rf, y_test_emo_rf)
  nnModel, nnHistory, x_testcnn, nn_acc = NeuralNetwork(X_train_nn, X_test_nn, y_train_emo_nn, y_test_emo_nn, 7, "emotion", 'emovo')

  #add to list
  emovo_dt_accuracies[int(i/2)] = dt_acc
  emovo_rf_accuracies[int(i/2)] = rf_acc
  emovo_nn_accuracies[int(i/2)] = nn_acc
  emovo_treeModels.append(treeModel)
  emovo_forestModels.append(rfModel)
  emovo_nnModels.append(nnModel)

print(emovo_dt_accuracies)
print(emovo_rf_accuracies)
print(emovo_nn_accuracies)

#savee
#path and file list
dataset_name = 'savee'
path = '/content/drive/My Drive/Senior/1980/' + dataset_name
lst = []

for subdir, dirs, files in os.walk(path):
    for file in files:
        try:
          #Load librosa array, obtain mfcss, store the file and the mcss information in a new array
          X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')
          mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) 
          #The emotion label for each audio file is saved in a different portion of the filename for
          #each dataset. Must subset appropriately
          if(file[0] == "s"):
              file = file[0:2]
          else:
              file = file[0]
          
          arr = mfccs, file
          lst.append(arr)
        # If the file is not valid, skip it
        except ValueError:
          continue

with open('savee.pkl', 'wb') as f:
  pickle.dump(lst, f)

#lists of accuracies
savee_rf_accuracies = np.zeros(5)
savee_dt_accuracies = np.zeros(5)
savee_nn_accuracies = np.zeros(5)

#list of models
savee_treeModels = []
savee_forestModels = []
savee_nnModels = []

#features to remove
dt_features_list = [0,2,1,30,39,38,35,5,33,34,31,29,17,6,4,18,28,8,19,21]
rf_features_list = [0,30,39,35,6,34,38,17,37,12,15,2,16,33,36,1,32,14,25,20]
nn_features_list = [2,4,39,38,19,17,1,33,0,7,34,18,20,9,37,10,15,13,11,31]

#get full arrays
X, y_emotion = zip(*lst)
X = np.asarray(X)
y_emotion = np.asarray(y_emotion)

X_train, X_test, y_train_emo, y_test_emo = train_test_split(X, y_emotion,test_size=0.33,random_state=42)

#for each set of features removed
for i in range(0, 10, 2):

  #indices of features removed
  dt_ind = dt_features_list[0:i]
  rf_ind = rf_features_list[0:i]
  nn_ind = nn_features_list[0:i]

  #modified training data
  X_mod_dt = X.copy()
  X_mod_rf = X.copy()
  X_mod_nn = X.copy()

  #remove features
  X_mod_dt[:,dt_ind] = 0
  X_mod_rf[:,rf_ind] = 0
  X_mod_nn[:,nn_ind] = 0

  #split into training and testing data
  X_train_dt, X_test_dt, y_train_emo_dt, y_test_emo_dt = train_test_split(X_mod_dt, y_emotion, test_size=0.33,random_state=123)
  X_train_rf, X_test_rf, y_train_emo_rf, y_test_emo_rf = train_test_split(X_mod_rf, y_emotion,  test_size=0.33,random_state=123)
  X_train_nn, X_test_nn, y_train_emo_nn, y_test_emo_nn = train_test_split(X_mod_nn, y_emotion, test_size=0.33,random_state=123)

  #train models
  treeModel, dt_acc = DecisionTree(X_train_dt, X_test_dt, y_train_emo_dt, y_test_emo_dt)
  rfModel, rf_acc = RandomForest(X_train_rf, X_test_rf, y_train_emo_rf, y_test_emo_rf)
  nnModel, nnHistory, x_testcnn, nn_acc = NeuralNetwork(X_train_nn, X_test_nn, y_train_emo_nn, y_test_emo_nn, 7, "emotion", 'savee')

  #add to list
  savee_dt_accuracies[int(i/2)] = dt_acc
  savee_rf_accuracies[int(i/2)] = rf_acc
  savee_nn_accuracies[int(i/2)] = nn_acc
  savee_treeModels.append(treeModel)
  savee_forestModels.append(rfModel)
  savee_nnModels.append(nnModel)

print(savee_dt_accuracies)
print(savee_rf_accuracies)
print(savee_nn_accuracies)
